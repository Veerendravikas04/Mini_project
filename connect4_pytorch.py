# -*- coding: utf-8 -*-
"""connect4-final-levels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12DAjcfvvHOz-axE2yhKf9RjRI9KvEeX-
"""

import numpy as np
import random
import os
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim

# Define the Neural Network model using PyTorch
class DQNetwork(nn.Module):
    def __init__(self, state_size=42, action_size=7):
        super(DQNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

def check_win(board, player):
    # Check horizontal
    for row in range(6):
        for col in range(4):
            if board[row][col] == player and board[row][col+1] == player and board[row][col+2] == player and board[row][col+3] == player:
                return True
    # Check vertical
    for row in range(3):
        for col in range(7):
            if board[row][col] == player and board[row+1][col] == player and board[row+2][col] == player and board[row+3][col] == player:
                return True
    # Check diagonal (positive slope)
    for row in range(3):
        for col in range(4):
            if board[row][col] == player and board[row+1][col+1] == player and board[row+2][col+2] == player and board[row+3][col+3] == player:
                return True
    # Check diagonal (negative slope)
    for row in range(3, 6):
        for col in range(4):
            if board[row][col] == player and board[row-1][col+1] == player and board[row-2][col+2] == player and board[row-3][col+3] == player:
                return True
    return False

def get_valid_columns(board):
    # Handle both list of lists and numpy arrays
    if isinstance(board, np.ndarray):
        return [col for col in range(7) if board[0, col] == 0]
    else:
        return [col for col in range(7) if board[0][col] == 0]

def apply_action(board, column, player):
    new_board = [row.copy() for row in board]
    for row in reversed(range(6)):
        if new_board[row][column] == 0:
            new_board[row][column] = player
            break
    return new_board

def preprocess_board(board):
    return np.array(board).flatten() / 2.0  # Normalize to [0, 0.5, 1]

class Connect4Env:
    def __init__(self):
        self.board = [[0 for _ in range(7)] for _ in range(6)]

    def reset(self):
        self.board = [[0 for _ in range(7)] for _ in range(6)]
        return self.board

    def step(self, action, player):
        new_board = apply_action(self.board, action, player)
        reward = 0
        done = False

        if check_win(new_board, player):
            reward = 1 if player == 2 else -1
            done = True
            self.board = new_board
            return self.board, reward, done

        if player == 2:
            opponent = 1
            opponent_can_win = False
            for a in get_valid_columns(new_board):
                temp_board = apply_action(new_board, a, opponent)
                if check_win(temp_board, opponent):
                    opponent_can_win = True
                    break
            if opponent_can_win:
                reward = -0.5
            else:
                reward = 0.1

        self.board = new_board
        return self.board, reward, done

class DQNAgent:
    def __init__(self, state_size=42, action_size=7, device="cpu"):
        self.state_size = state_size
        self.action_size = action_size
        self.device = device

        self.model = DQNetwork(state_size, action_size).to(device)
        self.target_model = DQNetwork(state_size, action_size).to(device)
        self.update_target_model()

        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()

        self.replay_buffer = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.995
        self.batch_size = 32

        # Training stats for saving
        self.total_episodes = 0

    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done):
        self.replay_buffer.append((state, action, reward, next_state, done))

    def act(self, state, valid_actions):
        if np.random.rand() <= self.epsilon:
            return random.choice(valid_actions)

        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        with torch.no_grad():
            q_values = self.model(state_tensor).cpu().numpy()[0]

        q_valid = [-float('inf')] * self.action_size
        for a in valid_actions:
            q_valid[a] = q_values[a]
        return np.argmax(q_valid)

    def train(self):
        if len(self.replay_buffer) < self.batch_size:
            return

        minibatch = random.sample(self.replay_buffer, self.batch_size)

        states = np.array([x[0] for x in minibatch])
        actions = np.array([x[1] for x in minibatch])
        rewards = np.array([x[2] for x in minibatch])
        next_states = np.array([x[3] for x in minibatch])
        dones = np.array([x[4] for x in minibatch])

        # Convert to PyTorch tensors
        states_tensor = torch.FloatTensor(states).to(self.device)
        next_states_tensor = torch.FloatTensor(next_states).to(self.device)
        actions_tensor = torch.LongTensor(actions).unsqueeze(1).to(self.device)
        rewards_tensor = torch.FloatTensor(rewards).to(self.device)
        dones_tensor = torch.FloatTensor(dones).to(self.device)

        # Current Q values
        current_q = self.model(states_tensor).gather(1, actions_tensor)

        # Next Q values from target model
        with torch.no_grad():
            next_q = torch.zeros(self.batch_size, device=self.device)
            for i in range(self.batch_size):
                if dones[i]:
                    next_q[i] = rewards[i]
                else:
                    # Convert next state to board format
                    next_board = (next_states[i].reshape(6, 7) * 2).astype(int)
                    valid_next_actions = get_valid_columns(next_board)
                    if valid_next_actions:
                        next_q_values = self.target_model(next_states_tensor[i:i+1]).cpu().numpy()[0]
                        valid_q_values = [next_q_values[a] for a in valid_next_actions]
                        next_max = max(valid_q_values)
                        next_q[i] = rewards[i] + self.gamma * next_max
                    else:
                        next_q[i] = rewards[i]

        # Calculate target Q values
        target_q = next_q.unsqueeze(1)

        # Compute loss
        loss = self.criterion(current_q, target_q)

        # Optimize the model
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def save(self, filename):
      # Convert replay buffer to a list for better compatibility
      replay_list = list(self.replay_buffer)

      checkpoint = {
          'model_state_dict': self.model.state_dict(),
          'target_model_state_dict': self.target_model.state_dict(),
          'optimizer_state_dict': self.optimizer.state_dict(),
          'epsilon': self.epsilon,
          'replay_buffer': replay_list,
          'total_episodes': self.total_episodes
      }
      torch.save(checkpoint, filename)
      print(f"Model saved to {filename}")

    def load(self, filename):
      if os.path.exists(filename):
          try:
              # Explicitly set weights_only=False for PyTorch 2.6+ compatibility
              checkpoint = torch.load(filename, map_location=self.device, weights_only=False)

              self.model.load_state_dict(checkpoint['model_state_dict'])
              self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
              self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
              self.epsilon = checkpoint['epsilon']

              # Only restore replay buffer if it exists in checkpoint
              if 'replay_buffer' in checkpoint:
                  self.replay_buffer = deque(maxlen=2000)
                  for item in checkpoint['replay_buffer']:
                      self.replay_buffer.append(item)

              # Load training stats
              if 'total_episodes' in checkpoint:
                  self.total_episodes = checkpoint['total_episodes']

              print(f"Model loaded from {filename}")
              print(f"Resumed from episode {self.total_episodes} with epsilon {self.epsilon:.4f}")
              return True
          except Exception as e:
              print(f"Error loading model: {e}")
              print("Starting with a fresh model state.")
              return False
      return False

def select_action(board, agent, difficulty):
    valid_actions = get_valid_columns(board)

    # Determine behavior based on difficulty
    if difficulty == "easy":
        # Easy mode: 70% random, 30% smart
        if np.random.random() < 0.7:
            return random.choice(valid_actions)
        # For the remaining 30%, use basic strategy

        # Check for immediate win
        for a in valid_actions:
            temp_board = apply_action(board, a, 2)
            if check_win(temp_board, 2):
                return a

        # 50% chance to miss blocking the opponent's win
        if np.random.random() < 0.5:
            # Check for opponent's immediate win
            for a in valid_actions:
                temp_board = apply_action(board, a, 1)
                if check_win(temp_board, 1):
                    return a

        # Otherwise random move
        return random.choice(valid_actions)

    elif difficulty == "moderate":
        # Moderate mode: 30% random, 70% smart
        if np.random.random() < 0.3:
            return random.choice(valid_actions)

        # Check for immediate win
        for a in valid_actions:
            temp_board = apply_action(board, a, 2)
            if check_win(temp_board, 2):
                return a

        # Check for opponent's immediate win
        for a in valid_actions:
            temp_board = apply_action(board, a, 1)
            if check_win(temp_board, 1):
                return a

        # Use the trained agent with some exploration
        state = preprocess_board(board)
        return agent.act(state, valid_actions)

    else:  # difficult
        # Hard mode: Always check for win/block, then use trained agent with minimal exploration

        # Check for immediate win
        for a in valid_actions:
            temp_board = apply_action(board, a, 2)
            if check_win(temp_board, 2):
                return a

        # Check for opponent's immediate win to block
        for a in valid_actions:
            temp_board = apply_action(board, a, 1)
            if check_win(temp_board, 1):
                return a

        # Check for two-move win (set up double threat)
        for a in valid_actions:
            temp_board = apply_action(board, a, 2)
            temp_valid = get_valid_columns(temp_board)
            wins_possible = 0
            for next_a in temp_valid:
                next_board = apply_action(temp_board, next_a, 2)
                if check_win(next_board, 2):
                    wins_possible += 1
            # If we found a move that creates two or more winning positions
            if wins_possible >= 2:
                return a

        # Use the trained agent with minimal exploration (0.05 epsilon)
        if np.random.random() < 0.05:
            return random.choice(valid_actions)
        else:
            state = preprocess_board(board)
            # Use the DQN agent for advanced strategy
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
            with torch.no_grad():
                q_values = agent.model(state_tensor).cpu().numpy()[0]

            q_valid = [-float('inf')] * agent.action_size
            for a in valid_actions:
                q_valid[a] = q_values[a]
            return np.argmax(q_valid)

def train_agent(episodes=1000, model_filename="connect4_pytorch_model.pth", resume=False):
    env = Connect4Env()

    # Set the device for training (use CUDA if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Create the agent
    agent = DQNAgent(device=device)

    # Load existing model if resuming training
    if resume and os.path.exists(model_filename):
        agent.load(model_filename)
        start_episode = agent.total_episodes
    else:
        start_episode = 0

    for e in range(start_episode, start_episode + episodes):
        state = env.reset()
        state = preprocess_board(state)
        done = False
        total_reward = 0

        while not done:
            # Agent's turn
            valid_actions = get_valid_columns(env.board)
            if not valid_actions:
                break
            # During training, use the full DQN algorithm
            action = agent.act(state, valid_actions)
            new_board, reward, done = env.step(action, 2)
            next_state = preprocess_board(new_board)
            agent.remember(state, action, reward, next_state, done)
            total_reward += reward

            # Opponent's turn (random)
            if not done:
                valid_actions_opp = get_valid_columns(new_board)
                if valid_actions_opp:
                    opp_action = random.choice(valid_actions_opp)
                    new_board_opp, _, done_opp = env.step(opp_action, 1)
                    if check_win(new_board_opp, 1):
                        reward = -1
                        done = True
                        agent.remember(next_state, opp_action, reward, preprocess_board(new_board_opp), done)
                        total_reward += reward
                    else:
                        done = done_opp
                    state = preprocess_board(new_board_opp)

            agent.train()

        if e % 10 == 0:
            agent.update_target_model()
        if e % 100 == 0:
          print(f"Episode: {e+1}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}")

        # Save model every 100 episodes
        if (e + 1) % 100 == 0:
            agent.total_episodes = e + 1
            agent.save(model_filename)

    # Save final model
    agent.total_episodes = start_episode + episodes
    agent.save(model_filename)

    return agent

def play_human_vs_ai(agent, human_first=True, difficulty="moderate"):
    env = Connect4Env()
    board = env.reset()
    turn = 1 if human_first else 2  # 1 for human, 2 for AI

    print(f"{'You' if human_first else 'AI'} will go first!")
    print("Human: X, AI: O")
    print(f"AI Difficulty: {difficulty}")

    def print_board(board):
        print("\n")
        for row in board:
            print("|", end="")
            for cell in row:
                symbol = ' '
                if cell == 1:
                    symbol = 'X'
                elif cell == 2:
                    symbol = 'O'
                print(f" {symbol} ", end="|")
            print()
        print("  0   1   2   3   4   5   6")
        print("\n")

    # Game loop
    while True:
        print_board(board)

        # Check for draw
        valid_actions = get_valid_columns(board)
        if not valid_actions:
            print("Draw!")
            break

        if turn == 1:  # Human's turn
            try:
                col = int(input("Your move (0-6): "))
                while col not in valid_actions:
                    col = int(input("Invalid column. Try again: "))
            except ValueError:
                print("Please enter a valid number between 0-6.")
                continue

            board = apply_action(board, col, 1)

            if check_win(board, 1):
                print_board(board)
                print("You win!")
                break

            # Switch turn
            turn = 2

        else:  # AI's turn
            print("AI is thinking...")
            # Use the difficulty-based action selection
            action = select_action(board, agent, difficulty)
            board = apply_action(board, action, 2)
            print(f"AI plays column {action}")

            if check_win(board, 2):
                print_board(board)
                print("AI wins!")
                break

            # Switch turn
            turn = 1

def load_agent_for_play(model_filename="connect4_pytorch_model.pth"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    agent = DQNAgent(device=device)

    try:
        if os.path.exists(model_filename):
            # Explicitly set weights_only=False to handle the full checkpoint
            # For PyTorch 2.6+
            checkpoint = torch.load(model_filename, map_location=device, weights_only=False)

            agent.model.load_state_dict(checkpoint['model_state_dict'])
            agent.target_model.load_state_dict(checkpoint['target_model_state_dict'])
            agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            agent.epsilon = checkpoint['epsilon']

            # Handle replay buffer more safely
            if 'replay_buffer' in checkpoint:
                agent.replay_buffer = deque(maxlen=2000)
                if isinstance(checkpoint['replay_buffer'], list):
                    for item in checkpoint['replay_buffer']:
                        agent.replay_buffer.append(item)

            if 'total_episodes' in checkpoint:
                agent.total_episodes = checkpoint['total_episodes']

            print(f"Model loaded from {model_filename}")
            print(f"Resumed from episode {agent.total_episodes} with epsilon {agent.epsilon:.4f}")
            return agent
        else:
            print(f"Model file {model_filename} not found.")
            return None
    except Exception as e:
        print(f"Error loading model: {e}")
        print("Starting with a fresh agent instead.")
        agent.epsilon = 0.1  # Set low epsilon for mostly exploitation but some exploration
        return agent

def main():
    model_file = "connect4_pytorch_model.pth"

    print("="*50)
    print("Welcome to Connect4 vs AI!")
    print("="*50)
    print("Choose an option:")
    print("1. Train new AI")
    print("2. Continue training existing AI")
    print("3. Play against AI")

    choice = input("Enter your choice (1-3): ")

    if choice == '1':
        episodes = int(input("Enter number of training episodes: "))
        print(f"Training agent for {episodes} episodes...")
        agent = train_agent(episodes=episodes, model_filename=model_file, resume=False)
        print("Training complete!")

        play_again = input("Do you want to play against the trained AI? (y/n): ")
        if play_again.lower() == 'y':
            who_first = input("Who goes first? (1 for human, 2 for AI): ")
            human_first = who_first != '2'

            print("Select difficulty level:")
            print("1. Easy")
            print("2. Moderate")
            print("3. Difficult")
            difficulty_choice = input("Enter difficulty (1-3): ")

            difficulty = "easy"
            if difficulty_choice == '2':
                difficulty = "moderate"
            elif difficulty_choice == '3':
                difficulty = "difficult"

            play_human_vs_ai(agent, human_first, difficulty)

    elif choice == '2':
        if not os.path.exists(model_file):
            print(f"No existing model found at {model_file}. Starting fresh training.")
            resume = False
        else:
            resume = True

        episodes = int(input("Enter number of additional training episodes: "))
        print(f"Training agent for {episodes} more episodes...")
        agent = train_agent(episodes=episodes, model_filename=model_file, resume=resume)
        print("Training complete!")

        play_again = input("Do you want to play against the trained AI? (y/n): ")
        if play_again.lower() == 'y':
            who_first = input("Who goes first? (1 for human, 2 for AI): ")
            human_first = who_first != '2'

            print("Select difficulty level:")
            print("1. Easy")
            print("2. Moderate")
            print("3. Difficult")
            difficulty_choice = input("Enter difficulty (1-3): ")

            difficulty = "easy"
            if difficulty_choice == '2':
                difficulty = "moderate"
            elif difficulty_choice == '3':
                difficulty = "difficult"

            play_human_vs_ai(agent, human_first, difficulty)

    elif choice == '3':
        agent = load_agent_for_play(model_file)
        if agent:
            who_first = input("Who goes first? (1 for human, 2 for AI): ")
            human_first = who_first != '2'

            print("Select difficulty level:")
            print("1. Easy")
            print("2. Moderate")
            print("3. Difficult")
            difficulty_choice = input("Enter difficulty (1-3): ")

            difficulty = "easy"
            if difficulty_choice == '2':
                difficulty = "moderate"
            elif difficulty_choice == '3':
                difficulty = "difficult"

            play_human_vs_ai(agent, human_first, difficulty)
        else:
            print("Failed to load agent or no trained model exists.")
            train_first = input("Would you like to train an agent first? (y/n): ")
            if train_first.lower() == 'y':
                episodes = int(input("Enter number of training episodes: "))
                print(f"Training agent for {episodes} episodes...")
                agent = train_agent(episodes=episodes, model_filename=model_file, resume=False)

                who_first = input("Who goes first? (1 for human, 2 for AI): ")
                human_first = who_first != '2'

                print("Select difficulty level:")
                print("1. Easy")
                print("2. Moderate")
                print("3. Difficult")
                difficulty_choice = input("Enter difficulty (1-3): ")

                difficulty = "easy"
                if difficulty_choice == '2':
                    difficulty = "moderate"
                elif difficulty_choice == '3':
                    difficulty = "difficult"

                play_human_vs_ai(agent, human_first, difficulty)
    else:
        print("Invalid choice.")

if __name__ == "__main__":
    main()

